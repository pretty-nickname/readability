{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JPWMzTDf5wI",
    "outputId": "aeb99002-d62b-4b2e-f780-3ab8ceb293df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "Hndn3UH8ivrY"
   },
   "outputs": [],
   "source": [
    "#training params\n",
    "batch_size = 256 \n",
    "num_epochs = 100\n",
    "\n",
    "#model parameters\n",
    "num_filters = 256\n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4\n",
    "network_type = 'LSTM' #'CNN'\n",
    "\n",
    "m = tf.keras.metrics.AUC(\n",
    "    num_thresholds=200,\n",
    "    curve=\"ROC\",\n",
    "    summation_method=\"interpolation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "-t10Isgxf-KG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_path = 'path'\n",
    "df = pd.read_csv(csv_path)\n",
    "train_texts = list(df.texts.values)\n",
    "train_labels = list(df.labels.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r2qTAzQ0oyus",
    "outputId": "3e58d78f-c60a-4fc2-b322-8b4c2721b45c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \").replace(\"«\", ' ').replace(\"»\", ' ').replace(\"…\", ' ')\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', ' ', text) \n",
    "    text = re.sub('[.,:;<>_%©?–*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = \" \".join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "    '''\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "      if word not in stops:\n",
    "        new_text.append(word)\n",
    "    \n",
    "    return ' '.join(new_text)\n",
    "    '''\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esnUk7-0pGD9",
    "outputId": "9537d2c3-87f0-400a-a118-947548864853"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low income countries will continue to be the most affected by human induced climate change over the next century they will experience gradual sea level rises stronger cyclones warmer days and nights more unpredictable rainfall and larger and longer heatwaves according to a recent report the last major united nations un assessment in predicted temperature rises of c or more by the end of the century that is now thought unlikely by scientists but average land and sea temperatures are expected to continue rising throughout this century possibly reaching c above present levels enough to devastate crops and make life in many cities unbearably hot as temperatures rise and oceans warm tropical and subtropical regions will see sharp changes in annual rainfall says the intergovernmental panel on climate change ipcc report released in stockholm and published online in september east africa can expect increased short rainfalls and west africa should expect heavier monsoons burma bangladesh and india can expect stronger cyclones elsewhere in southern asia heavier summer rains are anticipated indonesia may receive less rainfall between july and october but the coastal regions around the south china sea and gulf of thailand can expect increased rainfall extremes when cyclones hit the land in the long term rainfall patterns will change northern countries such as those in europe or north america are expected to receive more rainfall but many subtropical arid and semi arid regions will likely experience less rain said the reports authors they added that the monsoon season is likely to start earlier and last longer scientists in developing countries and commentators have welcomed the report which they said supported their own observations the ipcc says that climate change is real and happening much more strongly than before we are already seeing the effects of climate change in bangladesh and across south asia its not news to us most developing countries are facing climate change now they do not need the ipcc to tell them that the weather is changing said saleemul huq director of the international centre for climate change and development scientists have also lowered their projections of sea level rises depending on future greenhouse gas emissions sea levels will rise an average of cm by nevertheless there will be signifi cant geographical variations many millions of people living in the developing worlds great cities including lagos and calcutta are threatened weather disasters are also more likely in a warmer world the report suggests although the global frequency of tropical cyclones is expected to decrease or remain unchanged they may become more intense with stronger winds and heavier rainfall life in many developing country cities could become unbearable especially as urban temperatures are already far above those in surrounding countryside much higher temperatures could reduce the length of the growing period in some parts of africa by up to the report said the charity oxfam predicted that world hunger would worsen because climate changes inevitably hurt crop production and reduce incomes they said the number of people at risk of hunger might rise by to by the changing climate is already jeopardizing gains in the fi ght against hunger and it looks like it will get worse said oxfam a hot world is a hungry world ',\n",
       " 'the business idea is to produce a cheap light that gets free power from gravity and could end the use of dangerous kerosene lamps in africa and india but when british designer patrick hunt tried to get money from banks or venture capitalists to launch his invention he hit a problem we tried to get funding to make it happen but its slow and complex and its unproven and nobody wants to take a risk he said so he tried crowdfunding on a us website indiegogo which has recently opened in the uk within five days he hit his target and raised his campaign to get donations from the public was so popular that within days he had raised the led light is powered by a dynamo driven by a kg bag of rocks the weight is attached to the light lifted to a height of about m and while it slowly falls to the ground it will generate enough power for half an hour of light hunt is preparing for production in china and will test the market again by sending of the lights to africa before the full mass production of millions of units he is one of a new wave of entrepreneurs who are turning to the fast growing crowdfunding industry for money another new site is investingzone which matches wealthy people with start up entrepreneurs indiegogo does not offer shares but allows users to offer perks for different levels of investment people who helped to fund hunts light felt good about helping the less rich but also got their own light for danae ringelmann co founder of indiegogo the gravity light is a perfect example of how meritocratic crowdfunding can be and how it can test an entrepreneurs idea it is the first time that getting money has been fast efficient and meritocratic because it is not about how do i get access to the decision makers in that bank or who do i know in that venture capital company this is all about proving your worth to your customers and fans getting them to agree your idea will work and fund it even ideas that dont get funding are worth testing because you will have saved yourself a lot of time finding out it wasnt a good idea and getting smarter faster she says ringelmann who is based in the us started her career as a wall street analyst in she decided to quit and use her skills to try and help friends who worked in the arts to raise money five years on and the site is raising about m a week for new businesses in start up and growth stages in december it launched a euro and a sterling service to get a foothold on this side of the atlantic and says britain is its third biggest market international activity is up since december there is no shortage of competitors be it kickstarter seedrs or funding circle but indiegogo is the only crowdfunder where anyone can launch a campaign no project is thought too wacky the site charges a fee for successful campaigns for those that fail to raise their target amount users can either refund all money to their contributors at no charge or keep all money raised but pay a fee a british woman raised to open a cat caf in london through the site called lady dinahs cat emporium it is not open yet but is advertised as somewhere people can come in from the cold to a comfortable chair a hot cup of tea a book and a cat weve seen campaigns that go to venture capitalists get rejected because the venture capitalists say great idea but no idea if the market actually wants it it could be something that no one cares about said ringelmann the entrepreneurs do an indiegogo campaign and this can be enough market proof for venture capitalists to say there is a market for this it allows you to test your market test your pricing test your features discover new ways of getting money get vital feedback says ringelmann with her wall street background and the experience of helping businesses and services raise money ringelmann has useful advice for budding entrepreneurs ideas are a dime a dozen its all about the execution and if you are afraid that your idea will be stolen by someone who could execute it better and faster than you then you are not the right person to execute that idea its all about confidence to move fast and to learn she says while crowdfunding as an alternative to banks has grown it is not very attractive to big bucks investors who want a stake in a promising business that could start to change in the uk with the launch of investingzone ',\n",
       " 'when larry pizzi first heard about electric bikes nearly years ago he asked why would anyone want to spoil a bike by putting a motor and batteries on it its a question that some people still ask many bicycle shops in the us do not sell e bikes pizzi is ceo of currie technologies the number one seller of e bikes in the us he believes that things will change very soon other people who sell bikes agree familiar brands including trek raleigh and specialized all offer electric bikes and they believe that the market is going to grow the us is different from other countries when it comes to electric bikes nearly million e bikes were sold in most of them in china where people mostly use them for transportation they are popular in many parts of europe too theyre common in the netherlands and switzerland german postal workers use them and bmw sells one for about electric bikes are different from motorcycles or mopeds which use motors you pedal an electric bike with or without help from an electric motor riding an e bike feels like riding a normal bike with a strong wind behind you the motor just helps you go faster or climb hills you can usually ride e bicycles on bike lanes and they cant travel faster than mph e bikes are banned in some states in the us including new york some bike shops dont like putting motors on bicycles because it makes them too heavy some e bikes weigh nearly kg e bikes are also expensive while cheap bikes sell for just you will pay at least for a quality e bike with a good battery the best bikes cost more than but e bike technology particularly the batteries is improving batteries are getting smaller theyre getting lighter theyre getting more reliable and they are lasting longer says don dicostanza the ceo of pedego an electric bikemaker and retailer perhaps most importantly more cities are building bike lanes so bicycle commuting has become more popular electric bikes make commuting more practical and fun because people dont have to worry about hills strong winds tiredness and sweat most of our customers are baby boomers who want to have the cycling experience they had as a kid says pedegos don dicostanza the main reason they stopped riding bikes was because of hills pedego has opened nearly stores in the us electrobike has stores in mexico it opened its first american store in venice beach california in and hopes to have us stores in a year ceo craig anderson says we want to help reduce traffic help reduce our carbon footprint and encourage a healthy lifestyle he tells customers ride this bike once and try not to smile curries larry pizzi thinks that e bikes will become popular in north america a lot of young people are using e bikes for transportation instead of cars there is even a cargo bike with a stronger motor and rack at the back you can carry two children says pizzi you can carry kg of shopping its a minivan alternative ',\n",
       " 'much of bb kings best work was blues but he was always open minded about and interested in other kinds of music he bridged musical and cultural differences with warmth and skill perhaps it is too early to speak of the last of the bluesmen but it is hard to imagine that any future blues artist will match king he in uenced thousands of musicians and millions of music fans in a career that lasted years riley b king the b did not seem to stand for a name was born in mississippi the son of african american farm workers he learnt the basics of guitar from a family friend and perfected his singing with a quartet of gospel singers in his early s he moved to memphis within a couple of years he was playing regularly at a bar in west memphis and he also became a disc jockey presenting a show on a memphis radio station his billing the beale street blues boy was shortened to blues boy king and then to bb after a single session in for a nashville label king began recording for the west coast based modern records in he had his rst hit in with three oclock blues which was number one in the r b chart for weeks it was the rst of many hits on these and his dozens of other recordings most of them his own compositions king developed a style that was innovative but had its roots in blues history he was always ready to praise the musicians who had in uenced him he would usually mention t bone walker rst he would also cite the earlier blues guitarists blind lemon jefferson and lonnie johnson and the jazz players charlie christian and django reinhardt he once explained that his guitar technique was partly based on his lack of skill i started to bend notes because i could never play in the bottleneck style i loved that sound but just couldnt do it he was modest about his singing too a mixture of the style of ballad singers such as nat king cole and blues shouters such as joe turner and dr clayton probably his favourite composer and singer was louis jordan whose music he commemorated in the album let the good times roll throughout the s king was the leading blues artist on an endless series of one nighters in he played gigs in he tried to change that working pattern by signing with a major label abc but the rst records under that contract were rather unsuccessful both with his fans and with the record company the album live at the regal however has become iconic a turning point in the early listening of many younger musicians he had further r b hits with blues numbers including how blue can you get and paying the cost to be the boss and in he got near the top of the pop charts where no blues artist had been for many years with the thrill is gone it took him a while to establish himself with a rock audience but he was brought to their attention by musicians who admired him about a year and a half ago he said in all of a sudden kids started saying to me youre the greatest blues guitarist in the world and id say who told you that and theyd say mike bloom eld or eric clapton i owe my new popularity to these youngsters from then on king was rmly established as a leading blues artist guided by his manager sidney seidenberg he went on international concert tours that took him to japan australia china and russia he also gave concerts to prisoners at the cook county jail in chicago and at san quentin which led to his long involvement in rehabilitation programmes in king was diagnosed with diabetes and cut back his touring but his followers outside the us could still see him every year or two he would now deliver most of his act sitting down but the strength of his singing and the uency of his playing were almost as good as ever the celebrations for his th birthday in included an award winning album of collaborations with clapton mark knop er roger daltrey gloria estefan and others tributes from musicians as diverse as bono amadou bagayoko and elton john and a goodbye tour that was not a goodbye at all in king received a grammy award for best traditional blues album for one kind favor in he performed at a concert at the white house where the us president barack obama joined him to sing sweet home chicago king was twice married and twice divorced he is survived by children by various partners four others died before him ',\n",
       " 'it is no longer legal to smoke a cigarette inside a bar in the worlds drinking capital new orleans louisiana many other cities have banned indoor smoking but new orleans is different it attracts tourists with a let the good times roll attitude an indoor smoking ban in new orleans could have unique consequences since hurricane katrina in new orleans city government has begun trying to turn down the volume a bit with the support of neighbourhood groups the city has begun policing bars and nightclubs more strictly while at the same time ghting to implement a new noise ordinance this is just the wrong time for something like this complains bar owner william walker who hates the anti smoking law forcing people outside the bar to smoke is going to increase the tension thats already there many of new orleanss best bars and live music venues are in quiet residential neighbourhoods this neighbourly coexistence is a big part of what makes new orleans different and charming but recently this unique social contract has become unacceptable for some people and the fate of new orleanss musical personality feels at risk martha wood lives beside a loud bar that hosts live music i bought the house partly because of the bar so i wont ever complain about the noise says wood who also manages a live music bar this bar became one of new orleanss rst ever bars legally banned from serving drinks to go after a series of noise issues in including complaints about the loud smokers outside the maple leaf club went smoke free voluntarily in the same happened at another club where artists had been demanding smoke free nights a lot of the performance venues were already starting to show that consideration to performers so i wish the city would have just let that happen instead of forcing the ban into every corner bar that doesnt host music says zalia beville manager of the all ways lounge luckily all ways has an outdoor patio unlike lost love lounge whose owner geoff douville loves the ban hed previously felt forced to live with smoke to keep his bar going theres no way i could have banned smoking in my bar without a ban throughout the whole city says douville people act like i have that choice as a business owner but if i make that rule customers walk down the block to a bar with smoking many small business owners also fear smoke free revenue loss smoker neil timms owns an english pub and met the smoking ban before in england back home in england every pub i knew closed within a year of the smoking ban remembers timms of the uks ban begun in he doesnt want his pub to close so hes spending money to build a patio but douville feels the ban could be a great business opportunity there are lots of people who would enjoy coming out to our bar with our food but would never come because they didnt want to smell like smoke for the next seven days were now an option for all those people nor does douville worry about noise complaints no court is going to say a bar is a nuisance after the city has rati ed a smoking ban that requires you to go outside he says councilwoman latoya cantrell who introduced the ban disagrees the responsibility is on the bar owner to keep their customers respectful outside as well she says the owners and bartenders need to tell them to go and have a smoke but be respectful to their communities the idea that we cant have clean air because it will cause noise problems is ridiculous we can have clean air without noise problems i think its about communication and creating partnerships between the communities and the businesses cantrell recognizes that the city is different how is new orleans different from the rest of the country new orleans is known as a place where you can relax and have fun she says new orleans needs to stand up and say we care about our people the most vulnerable people who are working in smoky conditions are the backbone of our hospitality industry which drives the economy in the state of louisiana many were worried that the police would not have time to enforce the ban so the health department will handle bar warnings and nes bar customers are encouraged to ll out a form or call and to include photographs of illegal smoking for this reason alone neil timms says hell comply with the ban i dont want someone to be sitting in the corner smoking and someone takes a photo and gets beaten up unworried geoff douville says that hes used to noise complaints by now you will see the nosy neighbours who complain about the noise now are going to be the same ones who wanted the smoking ban in the end douville shares cantrells optimism of course theyre going to complain he accepts but it doesnt mean theyre going to win ']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts = [clean_text(text) for text in train_texts]\n",
    "train_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zNh0PpdggM0W",
    "outputId": "bba96269-b8b2-4554-d5e0-f5a0f39c6f5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, LSTM, Bidirectional\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense, concatenate\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, math, codecs\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"/content/gdrive/MyDrive/fiction_previews/embs/wiki-news-300d-1M.vec.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"/content/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spkKglQViKEa",
    "outputId": "79a86fec-e290-44dc-9916-f0d14346eb52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999995it [02:00, 8289.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 999995 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load embeddings\n",
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('/content/1/wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "#f = codecs.open('/content/1/glove.6B.200d.txt', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nGt76ukCiUEc",
    "outputId": "bc2c244b-d9d9-47d9-a0f2-c1626494ce05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 567/567 [00:00<00:00, 7944.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  15524\n",
      "763\n"
     ]
    }
   ],
   "source": [
    "num_classes = 3\n",
    "\n",
    "MAX_NB_WORDS = 20000\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "\n",
    "print(\"pre-processing train data...\")\n",
    "processed_docs_train = []\n",
    "for doc in tqdm(train_texts):\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    tokens = doc.split()\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_docs_train.append(\" \".join(filtered))\n",
    "\n",
    "print(\"tokenizing input data...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs_train)  #leaky\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "max_seq_len = 0\n",
    "for t in word_seq_train:\n",
    "  if len(t)>max_seq_len:\n",
    "    max_seq_len = len(t)\n",
    "\n",
    "print(max_seq_len)\n",
    "\n",
    "#pad sequences\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1150um1hi0Wn",
    "outputId": "396cc1f9-104d-41a8-dc17-61964992588c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 5780\n"
     ]
    }
   ],
   "source": [
    "#embedding matrix\n",
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "nb_words = MAX_NB_WORDS\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "grIu1Peu7cX6"
   },
   "outputs": [],
   "source": [
    "# architecture\n",
    "def create_model(network_type):\n",
    "  if network_type == 'CNN':\n",
    "    print(\"training CNN ...\")\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words, embed_dim,\n",
    "              weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
    "    model.add(Conv1D(num_filters, 2, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(num_filters, 2, activation='relu', padding='same'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[m])\n",
    "    #model.summary()\n",
    "    \n",
    "  if network_type == 'LSTM':\n",
    "    print(\"training LSTM ...\")\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words, embed_dim,\n",
    "              weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
    "    #model.add(Bidirectional(LSTM(128, dropout=0.5, recurrent_dropout=0.2, return_sequences = True)))\n",
    "    model.add(Bidirectional(LSTM(128, dropout=0.5, recurrent_dropout=0.2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[m])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "fxf7dQUSjhCQ"
   },
   "outputs": [],
   "source": [
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20, verbose=1)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12VfdQuLjm9c",
    "outputId": "3bc4e120-69b6-49ac-d2af-25276bd386a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LSTM ...\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/100\n",
      "2/2 - 94s - loss: 1.1074 - auc_5: 0.5064 - val_loss: 1.1032 - val_auc_5: 0.5229\n",
      "Epoch 2/100\n",
      "2/2 - 19s - loss: 1.1001 - auc_5: 0.5457 - val_loss: 1.1040 - val_auc_5: 0.4994\n",
      "Epoch 3/100\n",
      "2/2 - 18s - loss: 1.1018 - auc_5: 0.5298 - val_loss: 1.1042 - val_auc_5: 0.4986\n",
      "Epoch 4/100\n",
      "2/2 - 19s - loss: 1.0987 - auc_5: 0.5460 - val_loss: 1.1043 - val_auc_5: 0.5096\n",
      "Epoch 5/100\n",
      "2/2 - 19s - loss: 1.0984 - auc_5: 0.5493 - val_loss: 1.1045 - val_auc_5: 0.4976\n",
      "Epoch 6/100\n",
      "2/2 - 19s - loss: 1.1003 - auc_5: 0.5289 - val_loss: 1.1043 - val_auc_5: 0.4926\n",
      "Epoch 7/100\n",
      "2/2 - 18s - loss: 1.0942 - auc_5: 0.5598 - val_loss: 1.1040 - val_auc_5: 0.5024\n",
      "Epoch 8/100\n",
      "2/2 - 19s - loss: 1.0827 - auc_5: 0.6129 - val_loss: 1.1037 - val_auc_5: 0.5013\n",
      "Epoch 9/100\n",
      "2/2 - 19s - loss: 1.0708 - auc_5: 0.6516 - val_loss: 1.1039 - val_auc_5: 0.5063\n",
      "Epoch 10/100\n",
      "2/2 - 18s - loss: 1.0783 - auc_5: 0.6039 - val_loss: 1.1035 - val_auc_5: 0.5096\n",
      "Epoch 11/100\n",
      "2/2 - 19s - loss: 1.0663 - auc_5: 0.6282 - val_loss: 1.0860 - val_auc_5: 0.5691\n",
      "Epoch 12/100\n",
      "2/2 - 19s - loss: 1.0418 - auc_5: 0.6823 - val_loss: 1.0432 - val_auc_5: 0.6485\n",
      "Epoch 13/100\n",
      "2/2 - 18s - loss: 1.0267 - auc_5: 0.6600 - val_loss: 0.9835 - val_auc_5: 0.6932\n",
      "Epoch 14/100\n",
      "2/2 - 19s - loss: 1.0156 - auc_5: 0.6785 - val_loss: 1.2705 - val_auc_5: 0.5535\n",
      "Epoch 15/100\n",
      "2/2 - 19s - loss: 1.0384 - auc_5: 0.6719 - val_loss: 1.0317 - val_auc_5: 0.6478\n",
      "Epoch 16/100\n",
      "2/2 - 19s - loss: 1.0080 - auc_5: 0.6917 - val_loss: 1.0681 - val_auc_5: 0.6163\n",
      "Epoch 17/100\n",
      "2/2 - 19s - loss: 1.0334 - auc_5: 0.6552 - val_loss: 1.0751 - val_auc_5: 0.6014\n",
      "Epoch 18/100\n",
      "2/2 - 18s - loss: 1.0320 - auc_5: 0.6639 - val_loss: 1.0774 - val_auc_5: 0.5914\n",
      "Epoch 19/100\n",
      "2/2 - 19s - loss: 1.0261 - auc_5: 0.6926 - val_loss: 1.0878 - val_auc_5: 0.5535\n",
      "Epoch 20/100\n",
      "2/2 - 19s - loss: 1.0151 - auc_5: 0.7198 - val_loss: 1.1055 - val_auc_5: 0.5198\n",
      "Epoch 21/100\n",
      "2/2 - 18s - loss: 1.0086 - auc_5: 0.7061 - val_loss: 1.1139 - val_auc_5: 0.5154\n",
      "Epoch 22/100\n",
      "2/2 - 18s - loss: 1.0076 - auc_5: 0.6909 - val_loss: 1.0959 - val_auc_5: 0.5393\n",
      "Epoch 23/100\n",
      "2/2 - 19s - loss: 0.9861 - auc_5: 0.7296 - val_loss: 1.0704 - val_auc_5: 0.5815\n",
      "Epoch 24/100\n",
      "2/2 - 18s - loss: 0.9904 - auc_5: 0.7135 - val_loss: 2.0705 - val_auc_5: 0.4937\n",
      "Epoch 25/100\n",
      "2/2 - 18s - loss: 1.5335 - auc_5: 0.5394 - val_loss: 1.1742 - val_auc_5: 0.5924\n",
      "Epoch 26/100\n",
      "2/2 - 19s - loss: 1.1009 - auc_5: 0.6241 - val_loss: 1.0520 - val_auc_5: 0.6372\n",
      "Epoch 27/100\n",
      "2/2 - 18s - loss: 0.9605 - auc_5: 0.7202 - val_loss: 1.1387 - val_auc_5: 0.5699\n",
      "Epoch 28/100\n",
      "2/2 - 18s - loss: 0.9765 - auc_5: 0.7062 - val_loss: 1.2102 - val_auc_5: 0.5450\n",
      "Epoch 29/100\n",
      "2/2 - 18s - loss: 0.9668 - auc_5: 0.7215 - val_loss: 1.0817 - val_auc_5: 0.5847\n",
      "Epoch 30/100\n",
      "2/2 - 18s - loss: 0.9139 - auc_5: 0.7793 - val_loss: 1.0399 - val_auc_5: 0.6329\n",
      "Epoch 31/100\n",
      "2/2 - 18s - loss: 0.9230 - auc_5: 0.7617 - val_loss: 1.0253 - val_auc_5: 0.6537\n",
      "Epoch 32/100\n",
      "2/2 - 19s - loss: 0.9026 - auc_5: 0.7698 - val_loss: 1.0639 - val_auc_5: 0.6620\n",
      "Epoch 33/100\n",
      "2/2 - 19s - loss: 0.8443 - auc_5: 0.7943 - val_loss: 1.2027 - val_auc_5: 0.6580\n",
      "Epoch 00033: early stopping\n",
      "[68.46964590158781]\n",
      "training LSTM ...\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/100\n",
      "2/2 - 24s - loss: 1.1092 - auc_5: 0.5197 - val_loss: 1.1056 - val_auc_5: 0.4855\n",
      "Epoch 2/100\n",
      "2/2 - 18s - loss: 1.1045 - auc_5: 0.5068 - val_loss: 1.1035 - val_auc_5: 0.5017\n",
      "Epoch 3/100\n",
      "2/2 - 19s - loss: 1.1048 - auc_5: 0.5119 - val_loss: 1.1022 - val_auc_5: 0.5311\n",
      "Epoch 4/100\n",
      "2/2 - 19s - loss: 1.1007 - auc_5: 0.5318 - val_loss: 1.1007 - val_auc_5: 0.5351\n",
      "Epoch 5/100\n",
      "2/2 - 18s - loss: 1.1010 - auc_5: 0.5280 - val_loss: 1.0996 - val_auc_5: 0.5440\n",
      "Epoch 6/100\n",
      "2/2 - 18s - loss: 1.0995 - auc_5: 0.5351 - val_loss: 1.0999 - val_auc_5: 0.5405\n",
      "Epoch 7/100\n",
      "2/2 - 18s - loss: 1.0973 - auc_5: 0.5443 - val_loss: 1.1002 - val_auc_5: 0.5350\n",
      "Epoch 8/100\n",
      "2/2 - 18s - loss: 1.0942 - auc_5: 0.5533 - val_loss: 1.1007 - val_auc_5: 0.5289\n",
      "Epoch 9/100\n",
      "2/2 - 18s - loss: 1.0878 - auc_5: 0.5944 - val_loss: 1.1004 - val_auc_5: 0.5317\n",
      "Epoch 10/100\n",
      "2/2 - 18s - loss: 1.0901 - auc_5: 0.5757 - val_loss: 1.1024 - val_auc_5: 0.5108\n",
      "Epoch 11/100\n",
      "2/2 - 18s - loss: 1.0784 - auc_5: 0.6233 - val_loss: 1.1001 - val_auc_5: 0.5324\n",
      "Epoch 12/100\n",
      "2/2 - 19s - loss: 1.0695 - auc_5: 0.6332 - val_loss: 1.0934 - val_auc_5: 0.5532\n",
      "Epoch 13/100\n",
      "2/2 - 18s - loss: 1.1709 - auc_5: 0.5752 - val_loss: 1.1337 - val_auc_5: 0.5717\n",
      "Epoch 14/100\n",
      "2/2 - 19s - loss: 1.0921 - auc_5: 0.6069 - val_loss: 1.0858 - val_auc_5: 0.5858\n",
      "Epoch 15/100\n",
      "2/2 - 18s - loss: 1.0551 - auc_5: 0.6236 - val_loss: 1.0814 - val_auc_5: 0.5757\n",
      "Epoch 16/100\n",
      "2/2 - 18s - loss: 1.0468 - auc_5: 0.6760 - val_loss: 1.0968 - val_auc_5: 0.5326\n",
      "Epoch 17/100\n",
      "2/2 - 19s - loss: 1.0525 - auc_5: 0.6445 - val_loss: 1.0768 - val_auc_5: 0.5774\n",
      "Epoch 18/100\n",
      "2/2 - 18s - loss: 1.0269 - auc_5: 0.6994 - val_loss: 1.0457 - val_auc_5: 0.6304\n",
      "Epoch 19/100\n",
      "2/2 - 18s - loss: 1.0029 - auc_5: 0.6962 - val_loss: 1.0278 - val_auc_5: 0.6582\n",
      "Epoch 20/100\n",
      "2/2 - 19s - loss: 0.9589 - auc_5: 0.7245 - val_loss: 1.1447 - val_auc_5: 0.6288\n",
      "Epoch 21/100\n",
      "2/2 - 19s - loss: 1.0115 - auc_5: 0.7107 - val_loss: 1.2719 - val_auc_5: 0.6435\n",
      "Epoch 22/100\n",
      "2/2 - 19s - loss: 1.1912 - auc_5: 0.6469 - val_loss: 0.9897 - val_auc_5: 0.7099\n",
      "Epoch 23/100\n",
      "2/2 - 19s - loss: 0.9487 - auc_5: 0.7334 - val_loss: 1.0369 - val_auc_5: 0.6601\n",
      "Epoch 24/100\n",
      "2/2 - 18s - loss: 0.9791 - auc_5: 0.7221 - val_loss: 1.0902 - val_auc_5: 0.5951\n",
      "Epoch 25/100\n",
      "2/2 - 19s - loss: 1.0213 - auc_5: 0.6645 - val_loss: 1.0605 - val_auc_5: 0.6066\n",
      "Epoch 26/100\n",
      "2/2 - 19s - loss: 1.0074 - auc_5: 0.6991 - val_loss: 1.0499 - val_auc_5: 0.6047\n",
      "Epoch 27/100\n",
      "2/2 - 19s - loss: 0.9975 - auc_5: 0.7279 - val_loss: 1.0464 - val_auc_5: 0.6191\n",
      "Epoch 28/100\n",
      "2/2 - 18s - loss: 1.0022 - auc_5: 0.7346 - val_loss: 1.0405 - val_auc_5: 0.6311\n",
      "Epoch 29/100\n",
      "2/2 - 18s - loss: 0.9903 - auc_5: 0.7450 - val_loss: 1.0253 - val_auc_5: 0.6422\n",
      "Epoch 30/100\n",
      "2/2 - 19s - loss: 0.9759 - auc_5: 0.7412 - val_loss: 0.9732 - val_auc_5: 0.6741\n",
      "Epoch 31/100\n",
      "2/2 - 19s - loss: 0.9239 - auc_5: 0.7739 - val_loss: 1.0220 - val_auc_5: 0.6704\n",
      "Epoch 32/100\n",
      "2/2 - 19s - loss: 0.9315 - auc_5: 0.7558 - val_loss: 0.9450 - val_auc_5: 0.7355\n",
      "Epoch 33/100\n",
      "2/2 - 19s - loss: 0.8980 - auc_5: 0.7710 - val_loss: 0.8497 - val_auc_5: 0.7856\n",
      "Epoch 34/100\n",
      "2/2 - 18s - loss: 0.8400 - auc_5: 0.8034 - val_loss: 0.9529 - val_auc_5: 0.7192\n",
      "Epoch 35/100\n",
      "2/2 - 19s - loss: 0.9276 - auc_5: 0.7471 - val_loss: 1.0931 - val_auc_5: 0.6489\n",
      "Epoch 36/100\n",
      "2/2 - 18s - loss: 0.9576 - auc_5: 0.7293 - val_loss: 0.9763 - val_auc_5: 0.7047\n",
      "Epoch 37/100\n",
      "2/2 - 18s - loss: 0.8715 - auc_5: 0.7918 - val_loss: 1.0519 - val_auc_5: 0.6432\n",
      "Epoch 38/100\n",
      "2/2 - 18s - loss: 0.8749 - auc_5: 0.7993 - val_loss: 0.9937 - val_auc_5: 0.6730\n",
      "Epoch 39/100\n",
      "2/2 - 18s - loss: 0.8975 - auc_5: 0.7839 - val_loss: 0.9950 - val_auc_5: 0.6684\n",
      "Epoch 40/100\n",
      "2/2 - 19s - loss: 0.8910 - auc_5: 0.7788 - val_loss: 1.0358 - val_auc_5: 0.6555\n",
      "Epoch 41/100\n",
      "2/2 - 19s - loss: 0.8489 - auc_5: 0.8050 - val_loss: 1.0514 - val_auc_5: 0.6680\n",
      "Epoch 42/100\n",
      "2/2 - 18s - loss: 0.8153 - auc_5: 0.8163 - val_loss: 1.0072 - val_auc_5: 0.6959\n",
      "Epoch 43/100\n",
      "2/2 - 19s - loss: 0.8383 - auc_5: 0.8024 - val_loss: 0.9850 - val_auc_5: 0.7078\n",
      "Epoch 44/100\n",
      "2/2 - 19s - loss: 0.8979 - auc_5: 0.7666 - val_loss: 0.9971 - val_auc_5: 0.7056\n",
      "Epoch 45/100\n",
      "2/2 - 18s - loss: 0.8657 - auc_5: 0.7840 - val_loss: 1.0298 - val_auc_5: 0.7051\n",
      "Epoch 46/100\n",
      "2/2 - 18s - loss: 0.8867 - auc_5: 0.7834 - val_loss: 1.2351 - val_auc_5: 0.6424\n",
      "Epoch 47/100\n",
      "2/2 - 19s - loss: 0.9806 - auc_5: 0.7448 - val_loss: 1.0822 - val_auc_5: 0.6666\n",
      "Epoch 48/100\n",
      "2/2 - 18s - loss: 0.8723 - auc_5: 0.7850 - val_loss: 0.9064 - val_auc_5: 0.7586\n",
      "Epoch 49/100\n",
      "2/2 - 18s - loss: 0.7988 - auc_5: 0.8158 - val_loss: 0.9522 - val_auc_5: 0.7351\n",
      "Epoch 50/100\n",
      "2/2 - 19s - loss: 0.7917 - auc_5: 0.8238 - val_loss: 0.9705 - val_auc_5: 0.7232\n",
      "Epoch 51/100\n",
      "2/2 - 19s - loss: 0.8047 - auc_5: 0.8145 - val_loss: 0.9190 - val_auc_5: 0.7448\n",
      "Epoch 52/100\n",
      "2/2 - 19s - loss: 0.8155 - auc_5: 0.8055 - val_loss: 1.0500 - val_auc_5: 0.6590\n",
      "Epoch 53/100\n",
      "2/2 - 19s - loss: 0.8030 - auc_5: 0.8259 - val_loss: 1.1766 - val_auc_5: 0.6767\n",
      "Epoch 00053: early stopping\n",
      "[68.46964590158781, 53.14009661835749]\n",
      "training LSTM ...\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/100\n",
      "2/2 - 26s - loss: 1.1079 - auc_5: 0.5182 - val_loss: 1.1054 - val_auc_5: 0.4807\n",
      "Epoch 2/100\n",
      "2/2 - 19s - loss: 1.1030 - auc_5: 0.5151 - val_loss: 1.1093 - val_auc_5: 0.4780\n",
      "Epoch 3/100\n",
      "2/2 - 19s - loss: 1.1027 - auc_5: 0.5166 - val_loss: 1.1107 - val_auc_5: 0.4809\n",
      "Epoch 4/100\n",
      "2/2 - 18s - loss: 1.1009 - auc_5: 0.5296 - val_loss: 1.1104 - val_auc_5: 0.4537\n",
      "Epoch 5/100\n",
      "2/2 - 19s - loss: 1.1019 - auc_5: 0.5246 - val_loss: 1.1105 - val_auc_5: 0.4530\n",
      "Epoch 6/100\n",
      "2/2 - 19s - loss: 1.0951 - auc_5: 0.5697 - val_loss: 1.1105 - val_auc_5: 0.4251\n",
      "Epoch 7/100\n",
      "2/2 - 19s - loss: 1.0994 - auc_5: 0.5236 - val_loss: 1.1112 - val_auc_5: 0.4193\n",
      "Epoch 8/100\n",
      "2/2 - 18s - loss: 1.0895 - auc_5: 0.5927 - val_loss: 1.1130 - val_auc_5: 0.4289\n",
      "Epoch 9/100\n",
      "2/2 - 19s - loss: 1.0904 - auc_5: 0.5921 - val_loss: 1.1141 - val_auc_5: 0.4270\n",
      "Epoch 10/100\n",
      "2/2 - 19s - loss: 1.0833 - auc_5: 0.6179 - val_loss: 1.1153 - val_auc_5: 0.4113\n",
      "Epoch 11/100\n",
      "2/2 - 19s - loss: 1.0793 - auc_5: 0.6302 - val_loss: 1.1187 - val_auc_5: 0.4160\n",
      "Epoch 12/100\n",
      "2/2 - 18s - loss: 1.0728 - auc_5: 0.6532 - val_loss: 1.1313 - val_auc_5: 0.4193\n",
      "Epoch 13/100\n",
      "2/2 - 18s - loss: 1.0647 - auc_5: 0.6420 - val_loss: 1.1350 - val_auc_5: 0.4278\n",
      "Epoch 14/100\n",
      "2/2 - 19s - loss: 1.0500 - auc_5: 0.6702 - val_loss: 1.1483 - val_auc_5: 0.4851\n",
      "Epoch 15/100\n",
      "2/2 - 18s - loss: 1.0136 - auc_5: 0.6924 - val_loss: 1.1056 - val_auc_5: 0.5504\n",
      "Epoch 16/100\n",
      "2/2 - 18s - loss: 1.1000 - auc_5: 0.5921 - val_loss: 1.1266 - val_auc_5: 0.5200\n",
      "Epoch 17/100\n",
      "2/2 - 19s - loss: 1.0703 - auc_5: 0.6060 - val_loss: 1.1182 - val_auc_5: 0.4890\n",
      "Epoch 18/100\n",
      "2/2 - 18s - loss: 1.0437 - auc_5: 0.6704 - val_loss: 1.1323 - val_auc_5: 0.4591\n",
      "Epoch 19/100\n",
      "2/2 - 19s - loss: 1.0412 - auc_5: 0.6844 - val_loss: 1.1462 - val_auc_5: 0.4413\n",
      "Epoch 20/100\n",
      "2/2 - 19s - loss: 1.0478 - auc_5: 0.6584 - val_loss: 1.1578 - val_auc_5: 0.4225\n",
      "Epoch 21/100\n",
      "2/2 - 18s - loss: 1.0411 - auc_5: 0.6653 - val_loss: 1.1671 - val_auc_5: 0.4082\n",
      "Epoch 00021: early stopping\n",
      "[68.46964590158781, 53.14009661835749, 32.28216188999299]\n",
      "training LSTM ...\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 - 25s - loss: 1.1073 - auc_5: 0.4930 - val_loss: 1.1036 - val_auc_5: 0.5224\n",
      "Epoch 2/100\n",
      "2/2 - 19s - loss: 1.1084 - auc_5: 0.4800 - val_loss: 1.1053 - val_auc_5: 0.4956\n",
      "Epoch 3/100\n",
      "2/2 - 19s - loss: 1.1070 - auc_5: 0.4982 - val_loss: 1.1073 - val_auc_5: 0.4793\n",
      "Epoch 4/100\n",
      "2/2 - 19s - loss: 1.0997 - auc_5: 0.5423 - val_loss: 1.1099 - val_auc_5: 0.4589\n",
      "Epoch 5/100\n",
      "2/2 - 19s - loss: 1.0998 - auc_5: 0.5409 - val_loss: 1.1107 - val_auc_5: 0.4449\n",
      "Epoch 6/100\n",
      "2/2 - 18s - loss: 1.0981 - auc_5: 0.5514 - val_loss: 1.1096 - val_auc_5: 0.4549\n",
      "Epoch 7/100\n",
      "2/2 - 19s - loss: 1.0965 - auc_5: 0.5580 - val_loss: 1.1066 - val_auc_5: 0.4783\n",
      "Epoch 8/100\n",
      "2/2 - 19s - loss: 1.0872 - auc_5: 0.6052 - val_loss: 1.1039 - val_auc_5: 0.4935\n",
      "Epoch 9/100\n",
      "2/2 - 19s - loss: 1.0894 - auc_5: 0.6039 - val_loss: 1.1009 - val_auc_5: 0.5162\n",
      "Epoch 10/100\n",
      "2/2 - 19s - loss: 1.0890 - auc_5: 0.5855 - val_loss: 1.0988 - val_auc_5: 0.5165\n",
      "Epoch 11/100\n",
      "2/2 - 19s - loss: 1.0721 - auc_5: 0.6476 - val_loss: 1.0933 - val_auc_5: 0.5351\n",
      "Epoch 12/100\n",
      "2/2 - 19s - loss: 1.0667 - auc_5: 0.6586 - val_loss: 1.0811 - val_auc_5: 0.5722\n",
      "Epoch 13/100\n",
      "2/2 - 19s - loss: 1.0415 - auc_5: 0.6946 - val_loss: 1.0232 - val_auc_5: 0.6691\n",
      "Epoch 14/100\n",
      "2/2 - 19s - loss: 0.9947 - auc_5: 0.6993 - val_loss: 1.2672 - val_auc_5: 0.5395\n",
      "Epoch 15/100\n",
      "2/2 - 18s - loss: 1.0714 - auc_5: 0.6519 - val_loss: 1.1742 - val_auc_5: 0.6548\n",
      "Epoch 16/100\n",
      "2/2 - 19s - loss: 1.0797 - auc_5: 0.6602 - val_loss: 1.0319 - val_auc_5: 0.6606\n",
      "Epoch 17/100\n",
      "2/2 - 20s - loss: 1.0103 - auc_5: 0.6830 - val_loss: 1.0780 - val_auc_5: 0.6016\n",
      "Epoch 18/100\n",
      "2/2 - 19s - loss: 1.0180 - auc_5: 0.6709 - val_loss: 1.0681 - val_auc_5: 0.6004\n",
      "Epoch 19/100\n",
      "2/2 - 20s - loss: 1.0044 - auc_5: 0.7235 - val_loss: 1.0631 - val_auc_5: 0.5847\n",
      "Epoch 20/100\n",
      "2/2 - 19s - loss: 1.0100 - auc_5: 0.7215 - val_loss: 1.0625 - val_auc_5: 0.5997\n",
      "Epoch 21/100\n",
      "2/2 - 20s - loss: 0.9910 - auc_5: 0.7489 - val_loss: 1.0569 - val_auc_5: 0.6177\n",
      "Epoch 22/100\n",
      "2/2 - 19s - loss: 0.9954 - auc_5: 0.7245 - val_loss: 1.0417 - val_auc_5: 0.6353\n",
      "Epoch 23/100\n",
      "2/2 - 19s - loss: 0.9675 - auc_5: 0.7451 - val_loss: 1.0009 - val_auc_5: 0.6776\n",
      "Epoch 24/100\n",
      "2/2 - 19s - loss: 0.9143 - auc_5: 0.7700 - val_loss: 1.0414 - val_auc_5: 0.7029\n",
      "Epoch 25/100\n",
      "2/2 - 20s - loss: 0.9291 - auc_5: 0.7526 - val_loss: 0.9363 - val_auc_5: 0.7452\n",
      "Epoch 26/100\n",
      "2/2 - 20s - loss: 0.9121 - auc_5: 0.7535 - val_loss: 0.9036 - val_auc_5: 0.7576\n",
      "Epoch 27/100\n",
      "2/2 - 19s - loss: 0.8919 - auc_5: 0.7842 - val_loss: 0.9124 - val_auc_5: 0.7624\n",
      "Epoch 28/100\n",
      "2/2 - 20s - loss: 0.9776 - auc_5: 0.7222 - val_loss: 1.2745 - val_auc_5: 0.5665\n",
      "Epoch 29/100\n",
      "2/2 - 19s - loss: 1.2371 - auc_5: 0.5842 - val_loss: 1.2305 - val_auc_5: 0.5560\n",
      "Epoch 30/100\n",
      "2/2 - 20s - loss: 1.2003 - auc_5: 0.5653 - val_loss: 1.1614 - val_auc_5: 0.5520\n",
      "Epoch 31/100\n",
      "2/2 - 19s - loss: 1.1289 - auc_5: 0.5738 - val_loss: 1.1202 - val_auc_5: 0.5517\n",
      "Epoch 32/100\n",
      "2/2 - 20s - loss: 1.0996 - auc_5: 0.5736 - val_loss: 1.1025 - val_auc_5: 0.5535\n",
      "Epoch 33/100\n",
      "2/2 - 19s - loss: 1.0808 - auc_5: 0.5852 - val_loss: 1.0978 - val_auc_5: 0.5528\n",
      "Epoch 34/100\n",
      "2/2 - 20s - loss: 1.0706 - auc_5: 0.6107 - val_loss: 1.0988 - val_auc_5: 0.5396\n",
      "Epoch 35/100\n",
      "2/2 - 19s - loss: 1.0663 - auc_5: 0.6400 - val_loss: 1.1022 - val_auc_5: 0.5180\n",
      "Epoch 36/100\n",
      "2/2 - 20s - loss: 1.0619 - auc_5: 0.6555 - val_loss: 1.1050 - val_auc_5: 0.4992\n",
      "Epoch 37/100\n",
      "2/2 - 19s - loss: 1.0696 - auc_5: 0.6320 - val_loss: 1.1092 - val_auc_5: 0.4772\n",
      "Epoch 38/100\n",
      "2/2 - 19s - loss: 1.0706 - auc_5: 0.6187 - val_loss: 1.1136 - val_auc_5: 0.4646\n",
      "Epoch 39/100\n",
      "2/2 - 19s - loss: 1.0675 - auc_5: 0.6083 - val_loss: 1.1132 - val_auc_5: 0.4752\n",
      "Epoch 40/100\n",
      "2/2 - 19s - loss: 1.0562 - auc_5: 0.6544 - val_loss: 1.1111 - val_auc_5: 0.4941\n",
      "Epoch 41/100\n",
      "2/2 - 19s - loss: 1.0548 - auc_5: 0.6648 - val_loss: 1.1097 - val_auc_5: 0.5053\n",
      "Epoch 42/100\n",
      "2/2 - 19s - loss: 1.0558 - auc_5: 0.6667 - val_loss: 1.1085 - val_auc_5: 0.5060\n",
      "Epoch 43/100\n",
      "2/2 - 20s - loss: 1.0565 - auc_5: 0.6548 - val_loss: 1.1079 - val_auc_5: 0.5065\n",
      "Epoch 44/100\n",
      "2/2 - 20s - loss: 1.0470 - auc_5: 0.6818 - val_loss: 1.1080 - val_auc_5: 0.5100\n",
      "Epoch 45/100\n",
      "2/2 - 19s - loss: 1.0474 - auc_5: 0.6786 - val_loss: 1.1081 - val_auc_5: 0.5117\n",
      "Epoch 46/100\n",
      "2/2 - 19s - loss: 1.0327 - auc_5: 0.7089 - val_loss: 1.1082 - val_auc_5: 0.5126\n",
      "Epoch 00046: early stopping\n",
      "[68.46964590158781, 53.14009661835749, 32.28216188999299, 34.235868148453044]\n",
      "training LSTM ...\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/100\n",
      "2/2 - 26s - loss: 1.1046 - auc_5: 0.5178 - val_loss: 1.1119 - val_auc_5: 0.3555\n",
      "Epoch 2/100\n",
      "2/2 - 21s - loss: 1.1021 - auc_5: 0.5276 - val_loss: 1.1145 - val_auc_5: 0.3778\n",
      "Epoch 3/100\n",
      "2/2 - 21s - loss: 1.1002 - auc_5: 0.5397 - val_loss: 1.1174 - val_auc_5: 0.3848\n",
      "Epoch 4/100\n",
      "2/2 - 20s - loss: 1.0964 - auc_5: 0.5641 - val_loss: 1.1214 - val_auc_5: 0.3798\n",
      "Epoch 5/100\n",
      "2/2 - 19s - loss: 1.0941 - auc_5: 0.5724 - val_loss: 1.1248 - val_auc_5: 0.3575\n",
      "Epoch 6/100\n",
      "2/2 - 19s - loss: 1.0937 - auc_5: 0.5680 - val_loss: 1.1283 - val_auc_5: 0.3776\n",
      "Epoch 7/100\n",
      "2/2 - 20s - loss: 1.0902 - auc_5: 0.5816 - val_loss: 1.1306 - val_auc_5: 0.3881\n",
      "Epoch 8/100\n",
      "2/2 - 20s - loss: 1.0929 - auc_5: 0.5603 - val_loss: 1.1338 - val_auc_5: 0.3937\n",
      "Epoch 9/100\n",
      "2/2 - 19s - loss: 1.0820 - auc_5: 0.6005 - val_loss: 1.1352 - val_auc_5: 0.3774\n",
      "Epoch 10/100\n",
      "2/2 - 20s - loss: 1.0823 - auc_5: 0.6077 - val_loss: 1.1358 - val_auc_5: 0.3438\n",
      "Epoch 11/100\n",
      "2/2 - 20s - loss: 1.0673 - auc_5: 0.6554 - val_loss: 1.1330 - val_auc_5: 0.3690\n",
      "Epoch 12/100\n",
      "2/2 - 20s - loss: 1.0628 - auc_5: 0.6563 - val_loss: 1.1226 - val_auc_5: 0.4715\n",
      "Epoch 13/100\n",
      "2/2 - 19s - loss: 1.0374 - auc_5: 0.6857 - val_loss: 1.1200 - val_auc_5: 0.5423\n",
      "Epoch 14/100\n",
      "2/2 - 19s - loss: 1.0121 - auc_5: 0.6839 - val_loss: 1.3891 - val_auc_5: 0.4485\n",
      "Epoch 15/100\n",
      "2/2 - 20s - loss: 1.2664 - auc_5: 0.5223 - val_loss: 1.1755 - val_auc_5: 0.4090\n",
      "Epoch 16/100\n",
      "2/2 - 20s - loss: 1.1153 - auc_5: 0.5629 - val_loss: 1.2424 - val_auc_5: 0.4130\n",
      "Epoch 17/100\n",
      "2/2 - 20s - loss: 1.1580 - auc_5: 0.5510 - val_loss: 1.1862 - val_auc_5: 0.4479\n",
      "Epoch 18/100\n",
      "2/2 - 20s - loss: 1.1273 - auc_5: 0.5501 - val_loss: 1.1305 - val_auc_5: 0.4431\n",
      "Epoch 19/100\n",
      "2/2 - 20s - loss: 1.0719 - auc_5: 0.5987 - val_loss: 1.1228 - val_auc_5: 0.4517\n",
      "Epoch 20/100\n",
      "2/2 - 19s - loss: 1.0922 - auc_5: 0.5741 - val_loss: 1.1350 - val_auc_5: 0.4210\n",
      "Epoch 21/100\n",
      "2/2 - 19s - loss: 1.0843 - auc_5: 0.5862 - val_loss: 1.1431 - val_auc_5: 0.4355\n",
      "Epoch 00021: early stopping\n",
      "[68.46964590158781, 53.14009661835749, 32.28216188999299, 34.235868148453044, 40.17286508185518]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 42\n",
    "\n",
    "f1=[]\n",
    "recall=[]\n",
    "prec=[]\n",
    "\n",
    "for fold in range(5):\n",
    "  model = create_model(network_type)\n",
    "\n",
    "  texts_splitted = np.array_split(word_seq_train, 5)\n",
    "  classes_splitted = np.array_split(train_labels, 5)\n",
    "  test_fold_texts = texts_splitted[fold]\n",
    "  test_fold_classes = classes_splitted[fold]\n",
    "\n",
    "  train_fold_texts = []\n",
    "  train_fold_classes = []\n",
    "  for i,el in enumerate(texts_splitted):\n",
    "    if i!=fold:\n",
    "      train_fold_texts.extend(el)\n",
    "      train_fold_classes.extend(classes_splitted[i])\n",
    "\n",
    "  train_fold_classes = tf.keras.utils.to_categorical(train_fold_classes)\n",
    "  test_fold_classes = tf.keras.utils.to_categorical(test_fold_classes)\n",
    "\n",
    "  #model training\n",
    "  #callbacks=callbacks_list,\n",
    "  hist = model.fit(np.array(train_fold_texts), np.array(train_fold_classes), batch_size=batch_size, callbacks=callbacks_list,epochs=num_epochs, validation_split=0.1, shuffle=True, verbose=2)\n",
    "  y_predict= model.predict(np.array(test_fold_texts))\n",
    "\n",
    "  y_predict = y_predict.argmax(axis = 1)\n",
    "  test_fold_classes = test_fold_classes.argmax(axis=1)\n",
    "\n",
    "  f1.append(f1_score(y_predict, test_fold_classes, average='weighted')*100)\n",
    "  prec.append(precision_score(y_predict, test_fold_classes, average = 'weighted')*100)\n",
    "  recall.append(recall_score(y_predict, test_fold_classes, average = 'weighted')*100)\n",
    "\n",
    "  print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDlgzOAE8_2M",
    "outputId": "4e915c3e-a336-4794-8f38-204a6c8dbab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.555559395919445\n",
      "8.950460235908238\n",
      "37.884257715332\n",
      "12.27658143981291\n",
      "30.352429746933705\n",
      "6.5212865995919715\n"
     ]
    }
   ],
   "source": [
    "print(sum(f1)/5)\n",
    "print(np.std(np.array(f1)))\n",
    "print(sum(prec)/5)\n",
    "print(np.std(np.array(prec)))\n",
    "print(sum(recall)/5)\n",
    "print(np.std(np.array(recall)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_fasttext_baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
