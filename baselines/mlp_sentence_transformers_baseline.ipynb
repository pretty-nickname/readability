{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6b1suFSyOiU"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auDgHFy69v16"
   },
   "source": [
    "#add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqqjcfpeX9bP"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_path = \"path\"\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"/content/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZIoPLSnDPfy"
   },
   "outputs": [],
   "source": [
    "#additional features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#ONESTOPENGLISH - combining features\n",
    "\n",
    "corpus_path = \"\"\n",
    "df = pd.read_csv(corpus_path, header=None)\n",
    "additional_features_train = np.array(df.values)\n",
    "print(additional_features_train.shape)\n",
    "\n",
    "corpus_path = \"\"\n",
    "df = pd.read_csv(corpus_path, header = None)\n",
    "additional_features_train = np.hstack((additional_features_train, df.values))\n",
    "print(additional_features_train.shape)\n",
    "\n",
    "corpus_path = \"\"\n",
    "df = pd.read_csv(corpus_path, header = None)\n",
    "additional_features_train = np.hstack((additional_features_train, df.values))\n",
    "print(additional_features_train.shape)\n",
    "\n",
    "corpus_path = \"\"\n",
    "df = pd.read_csv(corpus_path)\n",
    "additional_features_train_ = df.iloc[:, 3:]\n",
    "additional_features_train_ = np.array(additional_features_train_.values)\n",
    "additional_features_train = np.hstack((additional_features_train, additional_features_train_))\n",
    "print(additional_features_train.shape)\n",
    "\n",
    "additional_features_train[np.isnan(additional_features_train)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbfXtyf-vuFX"
   },
   "outputs": [],
   "source": [
    "#fiction previews - combining features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/content/gdrive/MyDrive/fiction_previews/readability.csv\")\n",
    "additional_features_train = df.iloc[:, 1:]\n",
    "additional_features_train = np.array(additional_features_train.values)\n",
    "print(additional_features_train.shape)\n",
    "df = pd.read_csv(\"/content/gdrive/MyDrive/fiction_previews/morpho.csv\")\n",
    "additional_features_train_ = df.iloc[:, 1:]\n",
    "additional_features_train_ = np.array(additional_features_train_.values)\n",
    "additional_features_train = np.hstack((additional_features_train, additional_features_train_))\n",
    "print(additional_features_train.shape)\n",
    "df = pd.read_csv(\"/content/gdrive/MyDrive/fiction_previews/lda_fiction_previews.csv\", header = None)\n",
    "additional_features_train = np.hstack((additional_features_train, df.values))\n",
    "print(additional_features_train.shape)\n",
    "df = pd.read_csv(\"/content/gdrive/MyDrive/fiction_previews/punct_freq.csv\")\n",
    "additional_features_train_ = df.iloc[:, 3:]\n",
    "additional_features_train_ = np.array(additional_features_train_.values)\n",
    "additional_features_train = np.hstack((additional_features_train, additional_features_train_))\n",
    "print(additional_features_train.shape)\n",
    "\n",
    "df = pd.read_csv(\"/content/gdrive/MyDrive/fiction_previews/readability_test.csv\")\n",
    "additional_features_test = df.iloc[:, 1:]\n",
    "additional_features_test = np.array(additional_features_test.values)\n",
    "print(additional_features_test.shape)\n",
    "df = pd.read_csv(\"/content/gdrive/MyDrive/fiction_previews/morpho_test.csv\")\n",
    "additional_features_train_ = df.iloc[:, 1:]\n",
    "additional_features_train_ = np.array(additional_features_train_.values)\n",
    "additional_features_test = np.hstack((additional_features_test, additional_features_train_))\n",
    "print(additional_features_test.shape)\n",
    "df = pd.read_csv(\"/content/gdrive/MyDrive/fiction_previews/lda_fiction_previews_test.csv\", header = None)\n",
    "additional_features_test = np.hstack((additional_features_test, df.values))\n",
    "print(additional_features_test.shape)\n",
    "df = pd.read_csv(\"/content/gdrive/MyDrive/fiction_previews/punct_freq_test.csv\")\n",
    "additional_features_train_ = df.iloc[:, 3:]\n",
    "additional_features_train_ = np.array(additional_features_train_.values)\n",
    "additional_features_test = np.hstack((additional_features_test, additional_features_train_))\n",
    "print(additional_features_test.shape)\n",
    "\n",
    "additional_features_train[np.isnan(additional_features_train)] = 0\n",
    "additional_features_test[np.isnan(additional_features_test)] = 0\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "additional_features_train = scaler.fit_transform(additional_features_train)\n",
    "additional_features_test = scaler.transform(additional_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1juzY0oSqIhu"
   },
   "outputs": [],
   "source": [
    "#additional features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "csv_path = \"\"\n",
    "df = pd.read_csv(csv_path)\n",
    "additional_features_train = df.iloc[:, 64:]\n",
    "additional_features_train = np.array(additional_features_train.values)\n",
    "#additional_features_train = df.values\n",
    "additional_features_train[np.isnan(additional_features_train)] = 0\n",
    "#additional_features_test[np.isnan(additional_features_test)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RO-_e5UbgMJi"
   },
   "outputs": [],
   "source": [
    "#fiction_read\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "csv_path = \"\"\n",
    "df1 = pd.read_csv(csv_path)\n",
    "test_labels = list(df1.category.values)\n",
    "df1 = pd.read_csv(csv_path)\n",
    "train_labels = list(df1.category.values)\n",
    "\n",
    "print(train_labels[:10])\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "  if train_labels[i] == '1-2':\n",
    "    train_labels[i] = 0\n",
    "  if train_labels[i] == '3-4':\n",
    "    train_labels[i] = 1\n",
    "  if train_labels[i] == '5-7':\n",
    "    train_labels[i] = 2\n",
    "  if train_labels[i] == '8-9':\n",
    "    train_labels[i] = 3\n",
    "  if train_labels[i] == '10-11':\n",
    "    train_labels[i] = 4\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "  if test_labels[i] == '1-2':\n",
    "    test_labels[i] = 0\n",
    "  if test_labels[i] == '3-4':\n",
    "    test_labels[i] = 1\n",
    "  if test_labels[i] == '5-7':\n",
    "    test_labels[i] = 2\n",
    "  if test_labels[i] == '8-9':\n",
    "    test_labels[i] = 3\n",
    "  if test_labels[i] == '10-11':\n",
    "    test_labels[i] = 4\n",
    "\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/sent_trans_fiction_read_train.pickle', 'rb') as f:\n",
    "     train_texts = pickle.load(f)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/sent_trans_fiction_read_test.pickle', 'rb') as f:\n",
    "     test_texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtlh1LyY9iBn"
   },
   "outputs": [],
   "source": [
    "#kaggle\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "csv_path = \"\"\n",
    "df = pd.read_csv(csv_path)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/sent_trans_kaggle_train.pickle', 'rb') as f:\n",
    "     train_texts = pickle.load(f)\n",
    "train_labels = list(df.target.values)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/sent_trans_kaggle_test.pickle', 'rb') as f:\n",
    "     test_texts = pickle.load(f)\n",
    "test_labels = list(df.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YXbCQSQ3RVe"
   },
   "outputs": [],
   "source": [
    "#onestopenglish\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/sent_trans_onestopeng_train.pickle', 'rb') as f:\n",
    "     train_texts = pickle.load(f)\n",
    "csv_path = \"\"\n",
    "df = pd.read_csv(csv_path)\n",
    "train_labels = list(df.labels.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyJlJMco1QAQ"
   },
   "outputs": [],
   "source": [
    "#common_core\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/sent_trans_common_core_train.pickle', 'rb') as f:\n",
    "     train_texts = pickle.load(f)\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = \"\"\n",
    "df = pd.read_csv(csv_path)\n",
    "train_labels = list(df.Category.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dghNybnuyVFj"
   },
   "outputs": [],
   "source": [
    "#fiction_previews\n",
    "import pickle\n",
    "\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/train_fiction_previews_labels.pickle', 'rb') as f:\n",
    "     train_labels = pickle.load(f)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/test_fiction_previews_labels.pickle', 'rb') as f:\n",
    "     test_labels = pickle.load(f)\n",
    "print(len(train_labels), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnponPTcyr5e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/part5000.pickle', 'rb') as f:\n",
    "     train_texts= pickle.load(f)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/part10000.pickle', 'rb') as f:\n",
    "     train_texts = np.concatenate((train_texts, pickle.load(f)), axis=0)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/part15000.pickle', 'rb') as f:\n",
    "     train_texts = np.concatenate((train_texts, pickle.load(f)), axis=0)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/part20000.pickle', 'rb') as f:\n",
    "     train_texts = np.concatenate((train_texts, pickle.load(f)), axis=0)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/part25000.pickle', 'rb') as f:\n",
    "     train_texts = np.concatenate((train_texts, pickle.load(f)), axis=0)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/part30000.pickle', 'rb') as f:\n",
    "     train_texts = np.concatenate((train_texts, pickle.load(f)), axis=0)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/part35000.pickle', 'rb') as f:\n",
    "     train_texts = np.concatenate((train_texts, pickle.load(f)), axis=0)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/part40000.pickle', 'rb') as f:\n",
    "     train_texts = np.concatenate((train_texts, pickle.load(f)), axis=0)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/part45000.pickle', 'rb') as f:\n",
    "     train_texts = np.concatenate((train_texts, pickle.load(f)), axis=0)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/part.pickle', 'rb') as f:\n",
    "     train_texts = np.concatenate((train_texts, pickle.load(f)), axis=0)\n",
    "train_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQoib3LyC5mN"
   },
   "outputs": [],
   "source": [
    "with open('/content/gdrive/MyDrive/fiction_previews/partT5000.pickle', 'rb') as f:\n",
    "     test_texts= pickle.load(f)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/partT10000.pickle', 'rb') as f:\n",
    "     test_texts = np.concatenate((test_texts, pickle.load(f)), axis=0)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/partT.pickle', 'rb') as f:\n",
    "     test_texts = np.concatenate((test_texts, pickle.load(f)), axis=0)\n",
    "test_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPH0gDRA8kok"
   },
   "outputs": [],
   "source": [
    "#fiction_recommended\n",
    "import pickle\n",
    "\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/sent_trans_fiction_rec_train.pickle', 'rb') as f:\n",
    "     train_texts= pickle.load(f)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/sent_trans_fiction_rec_test.pickle', 'rb') as f:\n",
    "     test_texts= pickle.load(f)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/train_labels_fiction_recommended.pickle', 'rb') as f:\n",
    "     train_labels = pickle.load(f)\n",
    "with open('/content/gdrive/MyDrive/fiction_previews/test_labels_fiction_recommended.pickle', 'rb') as f:\n",
    "     test_labels = pickle.load(f)\n",
    "\n",
    "print(train_texts.shape, test_texts.shape, len(train_labels), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuCXPt2k57vb"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IP1h54SBDCBP"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-0.1,0.1))\n",
    "additional_features_train = scaler.fit_transform(additional_features_train)\n",
    "additional_features_test = scaler.transform(additional_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qx9pUb1_tz6_"
   },
   "outputs": [],
   "source": [
    "# combine main and additional features\n",
    "\n",
    "train_texts = np.hstack((train_texts,additional_features_train))\n",
    "test_texts = np.hstack((test_texts,additional_features_test))\n",
    "\n",
    "train_texts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiqlD0Eb96z_"
   },
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uef6suiK4GXL"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense, concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xWMDTq74w75"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_labels = tf.keras.utils.to_categorical(np.array(train_labels),3)\n",
    "#test_labels = tf.keras.utils.to_categorical(np.array(test_labels),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqKe-FkmUnaN"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_texts = scaler.fit_transform(train_texts)\n",
    "#test_texts = scaler.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZfPU57A3Evj"
   },
   "outputs": [],
   "source": [
    "train_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBs1WWQi5Gy4"
   },
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.AUC(\n",
    "    num_thresholds=200,\n",
    "    curve=\"ROC\",\n",
    "    summation_method=\"interpolation\",\n",
    ")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001) \n",
    "#opt = tf.keras.optimizers.SGD(\n",
    "    #learning_rate=0.1, momentum=0.0, nesterov=False, name=\"SGD\"\n",
    "#)\n",
    "\n",
    "\n",
    "#'''categorical_crossentropy\n",
    "inputs=Input(shape=(738,), name='input')\n",
    "x=Dense(1024, activation='tanh', name='fully_connected_1024_tanh')(inputs)\n",
    "x=Dense(1024, activation='tanh', name='fully_connected_32')(x)\n",
    "x=Dense(1024, activation='tanh', name='fully_connected_32_')(x)\n",
    "x=Dropout(0.5)(x)\n",
    "predictions=Dense(3, activation='softmax', name='output_softmax')(x)\n",
    "#predictions=Dense(1, activation='linear', name='output_softmax')(x)\n",
    "model=Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=[m])\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "model.summary()\n",
    "'''\n",
    "\n",
    "inputs=Input(shape=(512,), name='input')\n",
    "inputs_y=Input(shape=(len(list(additional_features_train[0])),), name='input_2', dtype='int64')\n",
    "\n",
    "x=Dense(1024, activation='tanh', name='fully_connected_1024_tanh')(inputs)\n",
    "x=Dense(1024, activation='tanh', name='fully_connected_32')(x)\n",
    "\n",
    "\n",
    "y=Dense(32, activation='tanh', name='fully_connected_')(inputs_y)\n",
    "y=Dense(32, activation='tanh', name='fully_connected__')(y)\n",
    "\n",
    "x=Model(inputs=inputs, outputs=x)\n",
    "y=Model(inputs=inputs_y, outputs=y)\n",
    "\n",
    "combined=concatenate([x.output, y.output], name='concat')\n",
    "ex=Dense(1024, activation='tanh', name='fully_connected_32_')(combined)\n",
    "ex=Dropout(0.5)(ex)\n",
    "predictions=Dense(2, activation='softmax', name='output_softmax')(ex)\n",
    "#predictions=Dense(1, activation='linear', name='output_softmax')(x)\n",
    "model=Model(inputs=[inputs, inputs_y], outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[m])\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDRbFxH836CI"
   },
   "outputs": [],
   "source": [
    "#one_stop_english\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 42\n",
    "\n",
    "f1=[]\n",
    "recall=[]\n",
    "prec=[]\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "import pickle\n",
    "path = \"\"\n",
    "with open(path + 'text_names.pickle', 'rb') as f:\n",
    "  list_of_texts = pickle.load(f)\n",
    "\n",
    "csv_path = \"\"\n",
    "df = pd.read_csv(csv_path)\n",
    "file_name = list(df.file_name.values)\n",
    "\n",
    "for fold in range(5):\n",
    "\n",
    "  inputs=Input(shape=(768,), name='input')\n",
    "  x=Dense(1024, activation='tanh', name='fully_connected_1024_tanh')(inputs)\n",
    "  x=Dense(1024, activation='tanh', name='fully_connected_32')(x)\n",
    "  x=Dense(1024, activation='tanh', name='fully_connected_32__')(x)\n",
    "  x=Dropout(0.5)(x)\n",
    "  predictions=Dense(3, activation='softmax', name='output_softmax')(x)\n",
    "  model=Model(inputs=inputs, outputs=predictions)\n",
    "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[m])\n",
    "\n",
    "  texts_splitted = np.array_split(list_of_texts, 5)\n",
    "  test_names = texts_splitted[fold]\n",
    "\n",
    "  train_fold_texts = []\n",
    "  train_fold_classes = []\n",
    "  test_fold_texts = []\n",
    "  test_fold_classes = []\n",
    "   \n",
    "  for i,el in enumerate(file_name):\n",
    "    is_test = False\n",
    "    for j, el1 in enumerate(test_names):\n",
    "      if el.find(el1)>=0:\n",
    "        test_fold_texts.append(train_texts[i])\n",
    "        test_fold_classes.append(train_labels[i])\n",
    "        is_test = True\n",
    "    if not is_test:\n",
    "      train_fold_texts.append(train_texts[i])\n",
    "      train_fold_classes.append(train_labels[i])\n",
    "\n",
    "  scaler = MinMaxScaler()\n",
    "  train_fold_texts = scaler.fit_transform(train_fold_texts)\n",
    "  test_texts = scaler.transform(test_fold_texts)\n",
    "\n",
    "  history = model.fit(np.array(train_fold_texts), np.array(train_fold_classes), epochs=100, shuffle=True, callbacks=callbacks_list, verbose=2, validation_split=0.1)\n",
    "  y_predict= model.predict(np.array(test_fold_texts))\n",
    "\n",
    "  y_predict = y_predict.argmax(axis = 1)\n",
    "  test_fold_classes = np.array(test_fold_classes).argmax(axis=1)\n",
    "\n",
    "  f1.append(f1_score(y_predict, test_fold_classes, average='weighted')*100)\n",
    "  prec.append(precision_score(y_predict, test_fold_classes, average = 'weighted')*100)\n",
    "  recall.append(recall_score(y_predict, test_fold_classes, average = 'weighted')*100)\n",
    "\n",
    "  print(f1)\n",
    "\n",
    "print(sum(f1)/5)\n",
    "print(np.std(np.array(f1)))\n",
    "print(sum(prec)/5)\n",
    "print(np.std(np.array(prec)))\n",
    "print(sum(recall)/5)\n",
    "print(np.std(np.array(recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5Q8VOSo2COV"
   },
   "outputs": [],
   "source": [
    "#common_core\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "seed = 42\n",
    "\n",
    "f1=[]\n",
    "recall=[]\n",
    "prec=[]\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "for fold in range(5):\n",
    "\n",
    "  inputs=Input(shape=(795,), name='input')\n",
    "  x=Dense(1024, activation='tanh', name='fully_connected_1024_tanh')(inputs)\n",
    "  x=Dense(1024, activation='tanh', name='fully_connected_32')(x)\n",
    "  x=Dense(1024, activation='tanh', name='fully_connected_32_')(x)\n",
    "  x=Dropout(0.5)(x)\n",
    "  predictions=Dense(6, activation='softmax', name='output_softmax')(x)\n",
    "  model=Model(inputs=inputs, outputs=predictions)\n",
    "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[m])\n",
    "\n",
    "  texts_splitted = np.array_split(train_texts, 5)\n",
    "  classes_splitted = np.array_split(train_labels, 5)\n",
    "  test_fold_texts = texts_splitted[fold]\n",
    "  test_fold_classes = classes_splitted[fold]\n",
    "\n",
    "  train_fold_texts = []\n",
    "  train_fold_classes = []\n",
    "  for i,el in enumerate(texts_splitted):\n",
    "    if i!=fold:\n",
    "      train_fold_texts.extend(el)\n",
    "      train_fold_classes.extend(classes_splitted[i])\n",
    "  '''\n",
    "  scaler = MinMaxScaler()\n",
    "  train_fold_texts = scaler.fit_transform(train_fold_texts)\n",
    "  test_texts = scaler.transform(test_fold_texts)\n",
    "  '''\n",
    "\n",
    "  history = model.fit(np.array(train_fold_texts), np.array(train_fold_classes), epochs=100, callbacks=callbacks_list, shuffle=True, verbose=2, validation_split=0.1)\n",
    "  y_predict= model.predict(np.array(test_fold_texts))\n",
    "\n",
    "  y_predict = y_predict.argmax(axis = 1)\n",
    "  test_fold_classes = test_fold_classes.argmax(axis=1)\n",
    "\n",
    "  f1.append(f1_score(y_predict, test_fold_classes, average='weighted')*100)\n",
    "  prec.append(precision_score(y_predict, test_fold_classes, average = 'weighted')*100)\n",
    "  recall.append(recall_score(y_predict, test_fold_classes, average = 'weighted')*100)\n",
    "\n",
    "  print(f1)\n",
    "\n",
    "print(sum(f1)/5)\n",
    "print(np.std(np.array(f1)))\n",
    "print(sum(prec)/5)\n",
    "print(np.std(np.array(prec)))\n",
    "print(sum(recall)/5)\n",
    "print(np.std(np.array(recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvOgZ4E43Pq0"
   },
   "outputs": [],
   "source": [
    "#kaggle\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "history = model.fit(np.array(train_texts), np.array(train_labels), epochs=100, callbacks=callbacks_list, verbose=2, validation_split=0.1)\n",
    "\n",
    "y_predict= model.predict(np.array(test_texts))\n",
    "\n",
    "print(mean_absolute_error(y_predict, test_labels))\n",
    "print(mean_squared_error(y_predict, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FifmXI9M-4D-"
   },
   "outputs": [],
   "source": [
    "#train_texts, test_texts = train_texts[:30000], train_texts[30000:]\n",
    "#train_labels, test_labels = train_labels[:30000], train_labels[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAbUGf5tknC5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "history = model.fit(train_texts, train_labels, epochs=100, callbacks=callbacks_list, verbose=2, shuffle=True, validation_split=0.1)\n",
    "#history = model.fit(train_texts, train_labels, epochs=30, verbose=2, shuffle=True, validation_split=0.1)\n",
    "predict = np.argmax(model.predict(test_texts), axis=1)\n",
    "answer = np.argmax(test_labels, axis=1)\n",
    "\n",
    "f1=f1_score(predict, answer, average = 'weighted')*100\n",
    "prec=precision_score(predict, answer, average = 'weighted')*100\n",
    "recall=recall_score(predict, answer, average = 'weighted')*100\n",
    "\n",
    "print('Готово!')\n",
    "print('f1 = {}, precision = {}, recall = {}'.format(f1,prec,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHsK2VfUz2tQ"
   },
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.AUC(\n",
    "    num_thresholds=200,\n",
    "    curve=\"ROC\",\n",
    "    summation_method=\"interpolation\",\n",
    ")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1) \n",
    "#opt = tf.keras.optimizers.SGD(\n",
    "    #learning_rate=0.1, momentum=0.0, nesterov=False, name=\"SGD\"\n",
    "#)\n",
    "\n",
    "\n",
    "#'''categorical_crossentropy\n",
    "inputs=Input(shape=(517,), name='input')\n",
    "x=Dense(1024, activation='tanh', name='fully_connected_1024_tanh')(inputs)\n",
    "x=Dense(1024, activation='tanh', name='fully_connected_32')(x)\n",
    "x=Dense(1024, activation='tanh', name='fully_connected_32_')(x)\n",
    "x=Dropout(0.5)(x)\n",
    "predictions=Dense(3, activation='softmax', name='output_softmax')(x)\n",
    "#predictions=Dense(1, activation='linear', name='output_softmax')(x)\n",
    "model=Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=opt, loss='poisson', metrics=[m])\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40aNBgGiz-L9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "history = model.fit(train_texts, train_labels, epochs=100, callbacks=callbacks_list, verbose=2, shuffle=True, validation_split=0.1)\n",
    "#history = model.fit(train_texts, train_labels, epochs=30, verbose=2, shuffle=True, validation_split=0.1)\n",
    "predict = np.argmax(model.predict(test_texts), axis=1)\n",
    "answer = np.argmax(test_labels, axis=1)\n",
    "\n",
    "f1=f1_score(predict, answer, average = 'weighted')*100\n",
    "prec=precision_score(predict, answer, average = 'weighted')*100\n",
    "recall=recall_score(predict, answer, average = 'weighted')*100\n",
    "\n",
    "print('Готово!')\n",
    "print('f1 = {}, precision = {}, recall = {}'.format(f1,prec,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRv9zSsF5XrK"
   },
   "outputs": [],
   "source": [
    "#all russian datasets\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "history = model.fit([train_texts,additional_features_train], train_labels, epochs=100, callbacks=callbacks_list, verbose=2, shuffle=True, validation_split=0.1)\n",
    "#history = model.fit(train_texts, train_labels, epochs=30, verbose=2, shuffle=True, validation_split=0.1)\n",
    "predict = np.argmax(model.predict([test_texts,additional_features_test]), axis=1)\n",
    "answer = np.argmax(test_labels, axis=1)\n",
    "\n",
    "f1=f1_score(predict, answer, average = 'weighted')*100\n",
    "prec=precision_score(predict, answer, average = 'weighted')*100\n",
    "recall=recall_score(predict, answer, average = 'weighted')*100\n",
    "\n",
    "print('Готово!')\n",
    "print('f1 = {}, precision = {}, recall = {}'.format(f1,prec,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jb9XchFptBqy"
   },
   "outputs": [],
   "source": [
    "predict = np.argmax(model.predict([test_texts,additional_features_test]), axis=1)\n",
    "answer = np.argmax(test_labels, axis=1)\n",
    "\n",
    "f1=f1_score(predict, answer, average = 'weighted')*100\n",
    "prec=precision_score(predict, answer, average = 'weighted')*100\n",
    "recall=recall_score(predict, answer, average = 'weighted')*100\n",
    "\n",
    "print('Готово!')\n",
    "print('f1 = {}, precision = {}, recall = {}'.format(f1,prec,recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIuNQTQ2-m5o"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predict, answer))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlp_sentence_transformers_baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
