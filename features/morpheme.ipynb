{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fiction_previews.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yeIgP-IIEtlh"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz8IIdgAKPGY"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "\n",
        "#read train_texts ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-sswwFH-gIl"
      },
      "source": [
        "#English readability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TiHw3o1fLCD"
      },
      "source": [
        "!pip install readability\n",
        "import readability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v6uZZFlffbT"
      },
      "source": [
        "import csv\n",
        "\n",
        "for i, text in enumerate(train_texts):\n",
        "  results = readability.getmeasures(text, lang='en')\n",
        "  with open(r'', 'a') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([results['readability grades']['Kincaid'],\\\n",
        "                 results['readability grades']['Coleman-Liau'],\\\n",
        "                 results['readability grades']['ARI'],\\\n",
        "                 results['readability grades']['SMOGIndex'],\\\n",
        "                 results['readability grades']['DaleChallIndex']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeIgP-IIEtlh"
      },
      "source": [
        "#Morphological"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfTHyfQ6YNar"
      },
      "source": [
        "#preprocessing\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "!pip install pymorphy2\n",
        "import pymorphy2, re\n",
        "ma = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def clean_text(text):#rus\n",
        "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \").replace(\"«\", '').replace(\"»\", '').replace(\"…\", '')\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', '', text) \n",
        "    text = re.sub('[.,:;<>_%©?–*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text)\n",
        "    text = \" \".join(ma.parse(word)[0].normal_form for word in text.split())\n",
        "    new_text = []\n",
        "    for word in text.split():\n",
        "      if word not in stops:\n",
        "        new_text.append(word)\n",
        "    return ' '.join(new_text)\n",
        "\n",
        "def clean_text_eng(text):#eng\n",
        "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \").replace(\"«\", ' ').replace(\"»\", ' ').replace(\"…\", ' ')\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', ' ', text) \n",
        "    text = re.sub('[.,:;<>_%©?–*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    #text = \" \".join(lemmatizer.lemmatize(word) for word in text.split())\n",
        "    #text = \" \".join(ma.parse(word)[0].normal_form for word in text.split())\n",
        "    '''\n",
        "    new_text = []\n",
        "    for word in text.split():\n",
        "      if word not in stops:\n",
        "        new_text.append(word)\n",
        "    \n",
        "    return ' '.join(new_text)\n",
        "    '''\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU5Yn1bWZ1OY"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "pos_list = ['CC', 'CD', 'DT', 'FW', 'EX', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT',\\\n",
        "            'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',\\\n",
        "            'WDT', 'WP', 'WRB', 'WP$']\n",
        "for j, text in enumerate(train_texts):\n",
        "  counts = np.zeros(len(pos_list))\n",
        "  text = clean_text_eng(text)\n",
        "  tokens=nltk.word_tokenize(text)\n",
        "  tokens = nltk.pos_tag(tokens)\n",
        "  for i, token in enumerate(tokens):\n",
        "    try:\n",
        "      pos = pos_list.index(tokens[i][1])\n",
        "      counts[pos] += 1\n",
        "    except:\n",
        "      print(tokens[i][1])\n",
        "  counts = [el/np.sum(counts) for el in counts]\n",
        "  with open(r'', 'a') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(counts)\n",
        "    \n",
        "for j, text in enumerate(test_texts):\n",
        "  counts = np.zeros(len(pos_list))\n",
        "  text = clean_text_eng(text)\n",
        "  tokens=nltk.word_tokenize(text)\n",
        "  tokens = nltk.pos_tag(tokens)\n",
        "  for i, token in enumerate(tokens):\n",
        "    try:\n",
        "      pos = pos_list.index(tokens[i][1])\n",
        "      counts[pos] += 1\n",
        "    except:\n",
        "      print(tokens[i][1])\n",
        "  counts = [el/np.sum(counts) for el in counts]\n",
        "  with open(r'', 'a') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INES9tjtE0R0"
      },
      "source": [
        "import numpy as np\n",
        "pos = ['NOUN', 'ADJF', 'ADJS', 'COMP', 'VERB', 'INFN', 'PRTF', 'PRTS', 'GRND', 'NUMR', 'ADVB',\\\n",
        "       'NPRO', 'PRED', 'PREP', 'CONJ', 'PRCL', 'INTJ']\n",
        "path = ''\n",
        "\n",
        "import csv   \n",
        "\n",
        "#'''\n",
        "labels = ['filename','morpho_NOUN', 'morpho_ADJF', 'morpho_ADJS', 'morpho_COMP', 'morpho_VERB', 'morpho_INFN', 'morpho_PRTF', 'morpho_PRTS', 'morpho_GRND',\\\n",
        "          'morpho_NUMR', 'morpho_ADVB', 'morpho_NPRO', 'morpho_PRED', 'morpho_PREP', 'morpho_CONJ', 'morpho_PRCL', 'morpho_INTJ',\\\n",
        "          'morpho_FUTR', 'morpho_PRES', 'morpho_PAST', 'morpho_ANIMACY', 'morph_PERF', 'morpho_TRANS', 'morpho_NOMN', 'morpho_GENT', 'morpho_DATV',\\\n",
        "          'morpho_ACCS', 'morpho_ABLT', 'morpho_LOCT', 'morpho_VOCT', 'morpho_GEN2', 'morpho_ACC2', 'morpho_LOC2']\n",
        "\n",
        "with open(r'', 'a') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(labels)\n",
        "#'''\n",
        "\n",
        "found = True\n",
        "\n",
        "'''\n",
        "for i, f in enumerate(files):\n",
        "  if found:\n",
        "    with open(path + f) as f1:\n",
        "        content = f1.read()\n",
        "      '''\n",
        "for i, text in enumerate(train_texts):\n",
        "    pos_count = np.zeros(len(pos) + 6)\n",
        "    #words = clean_text(content).split()\n",
        "    words = clean_text(text).split()\n",
        "\n",
        "    odush = 0\n",
        "    perf = 0\n",
        "    trans = 0\n",
        "    num_verbs_personal = 0\n",
        "    past = 0\n",
        "    pres = 0\n",
        "    futr = 0\n",
        "\n",
        "    cases = np.zeros(10)\n",
        "\n",
        "    for word in words:\n",
        "      try:\n",
        "        p = ma.parse(word)[0]\n",
        "        pos_tag = p.tag.POS\n",
        "        pos_count[pos.index(pos_tag)]+=1\n",
        "        if pos_tag == 'NOUN':\n",
        "          if p.tag.animacy == 'anim':\n",
        "            odush += 1\n",
        "\n",
        "          if p.tag.case == 'nomn':\n",
        "            cases[0] += 1\n",
        "          elif p.tag.case == 'gent':\n",
        "            cases[1] += 1\n",
        "          elif p.tag.case == 'datv':\n",
        "            cases[2] += 1\n",
        "          elif p.tag.case == 'accs':\n",
        "            cases[3] += 1\n",
        "          elif p.tag.case == 'ablt':\n",
        "            cases[4] += 1\n",
        "          elif p.tag.case == 'loct':\n",
        "            cases[5] += 1\n",
        "          elif p.tag.case == 'voct':\n",
        "            cases[6] += 1\n",
        "          elif p.tag.case == 'gen2':\n",
        "            cases[7] += 1\n",
        "          elif p.tag.case == 'acc2':\n",
        "            cases[8] += 1\n",
        "          elif p.tag.case == 'loc2':\n",
        "            cases[9] += 1\n",
        "\n",
        "        if pos_tag == 'VERB' or pos_tag == 'INFN':\n",
        "          if p.tag.aspect == 'perf':\n",
        "            perf += 1\n",
        "          if p.tag.transitivity == 'tran':\n",
        "            trans += 1\n",
        "          if p.tag.tense == 'futr':\n",
        "            futr += 1\n",
        "            num_verbs_personal += 1\n",
        "          elif p.tag.tense == 'past':\n",
        "            past += 1\n",
        "            num_verbs_personal += 1\n",
        "          elif p.tag.tense == 'pres':\n",
        "            pres += 1\n",
        "            num_verbs_personal += 1\n",
        "        #считаем одушевленные\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    num_nouns = pos_count[0]\n",
        "    num_verbs = pos_count[4] + pos_count[5]\n",
        "\n",
        "    pos_count = pos_count / len(words)\n",
        "    pos_count[len(pos_count)-1] = trans / num_verbs\n",
        "    pos_count[len(pos_count)-2] = perf / num_verbs\n",
        "    pos_count[len(pos_count)-3] = odush / num_nouns\n",
        "    if num_verbs_personal!=0:\n",
        "      pos_count[len(pos_count)-4] = past / num_verbs_personal\n",
        "      pos_count[len(pos_count)-5] = pres / num_verbs_personal\n",
        "      pos_count[len(pos_count)-6] = futr / num_verbs_personal\n",
        "    else:\n",
        "      pos_count[len(pos_count)-4] = 0\n",
        "      pos_count[len(pos_count)-5] = 0\n",
        "      pos_count[len(pos_count)-6] = 0\n",
        "\n",
        "    res = [f]\n",
        "    res.extend(list(np.concatenate((pos_count,cases/num_nouns))))\n",
        "\n",
        "    #print(np.concatenate((pos_count,cases/num_nouns)))\n",
        "    with open(r'', 'a') as f2:\n",
        "      writer = csv.writer(f2)\n",
        "      writer.writerow(res)\n",
        "\n",
        "    if i%1000==0:\n",
        "      print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-ecWHnHZg0O"
      },
      "source": [
        "#Russian readability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9AmUIu4ZjuQ"
      },
      "source": [
        "import requests\n",
        "\n",
        "import csv  \n",
        "\n",
        "found = True\n",
        "\n",
        "path = \"\"\n",
        "labels = [\"filename\", \"readability_index_cl\", \"readability_index_ari\", \"readability_index_dc\", \"readability_index_SMOG\", \"readability_index_fk\"]\n",
        "\n",
        "with open(r'', 'a') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(labels)\n",
        "\n",
        "for i, text in enumerate(train_texts):\n",
        "    try:\n",
        "      content = text\n",
        "      response = requests.post(\"https://api.plainrussian.ru/api/1.0/ru/measure/\", data={\"text\":content}).json()\n",
        "      new_line = [f, response['indexes']['index_cl'], response['indexes']['index_ari'], response['indexes']['index_dc'], \\\n",
        "                  response['indexes']['index_SMOG'], response['indexes']['index_fk']]\n",
        "\n",
        "      with open(r'', 'a') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(new_line)\n",
        "\n",
        "      if i%1000==0:\n",
        "          print(i)\n",
        "    except:\n",
        "      print(i, f)\n",
        "      new_line = [f]\n",
        "      with open(r'', 'a') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(new_line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zE5crv5io3A"
      },
      "source": [
        "#LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d92_NLeNizOM"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('russian')\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FruCM73EjVTt"
      },
      "source": [
        "train_texts = [clean_text_eng(text) for text in train_texts]\n",
        "test_texts = [clean_text_eng(text) for text in test_texts]\n",
        "train_texts = [el.split() for el in train_texts]\n",
        "test_texts = [el.split() for el in test_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW9cvzBWjoWI"
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(train_texts, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[train_texts], threshold=100)  \n",
        "bigram_ = gensim.models.Phrases(test_texts, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram_ = gensim.models.Phrases(bigram[test_texts], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "bigram_mod_ = gensim.models.phrases.Phraser(bigram_)\n",
        "trigram_mod_ = gensim.models.phrases.Phraser(trigram_)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[train_texts[0]]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMO296wnlugY"
      },
      "source": [
        "import spacy\n",
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qogz_Mjml-RX"
      },
      "source": [
        "import spacy\n",
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(train_texts)\n",
        "data_words_nostops_ = remove_stopwords(test_texts)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "data_words_bigrams_ = make_bigrams(data_words_nostops_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GmjU6DUl1Xy"
      },
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words_bigrams)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_words_bigrams\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "corpus_ = [id2word.doc2bow(text) for text in data_words_bigrams_]\n",
        "\n",
        "# View\n",
        "print(corpus[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx5OfS8W32iP"
      },
      "source": [
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=100, \n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           workers=3,\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCEyMrjmmQVc"
      },
      "source": [
        "def get_dist(dist):\n",
        "  new_dist = []\n",
        "  for d in dist:\n",
        "    new_dist.append(d[1])\n",
        "  return new_dist\n",
        "\n",
        "for doc in corpus:\n",
        "  with open(r'', 'a') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(get_dist(lda_model.get_document_topics(doc, minimum_probability=0.0)))\n",
        "\n",
        "for doc in corpus_:\n",
        "  with open(r'', 'a') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(get_dist(lda_model.get_document_topics(doc, minimum_probability=0.0)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}